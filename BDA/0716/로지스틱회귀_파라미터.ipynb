{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 로지스틱 회귀 파라미터 정리\n",
        "1. penalty : {‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’\n",
        "- 규제 방식 지정. 기본적으로 L2 규제 사용.\n",
        "2. dual : bool, default=False\n",
        "-  불리언(boolean) 파라미터로, 모델의 최적화 방법을 선택하는 데에 사용\n",
        "3. tol : float, default=1e-4\n",
        "- 수렴 기준으로 사용. 최적화 알고리즘은 반복적인 과정을 통해 손실 함수를 최소화하며, 이 때 손실 함수의 변화량이 일정 기준 이하로 내려가면 수렴되었다고 판단하고 학습을 종료한다. tol 파라미터는 이러한 수렴 기준을 설정하는 데에 사용\n",
        "4. C : float, default=1.0\n",
        "- 규제의 강도를 조절하는 파라미터. C 값이 작을수록 규제가 강화되며, 큰 값을 사용하면 규제가 약화됨.\n",
        "5. fit_intercept : bool, default=True\n",
        "- 불리언(boolean) 파라미터로, 모델에 절편(intercept)을 학습할지 여부를 결정\n",
        "6. solver : {‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs\n",
        "- 최적화에 사용되는 알고리즘을 지정\n",
        "7. max_iter : int, default=100\n",
        "- 최적화 알고리즘의 최대 반복 횟수를 제한. 최적화가 수렴하지 않을 경우 이 값을 늘려주어야 한다.\n",
        "8. verbose : int, default=0\n",
        "- 불리언(boolean) 파라미터로, 학습 과정 중에 진행 상황을 출력할지 여부를 결정하는 데에 사용\n",
        "9. warm_start : bool, default=False\n",
        "- 불리언(boolean) 파라미터로, 이전 학습 결과를 유지하고 추가적인 학습을 수행할지 여부를 결정하는 데에 사용\n",
        "10. random_state\n",
        "- 모델의 랜덤 시드를 지정. 동일한 랜덤 시드로 모델을 학습시키면 항상 동일한 결과를 얻을 수 있다."
      ],
      "metadata": {
        "id": "L2lPIKHzNzuE"
      }
    }
  ]
}